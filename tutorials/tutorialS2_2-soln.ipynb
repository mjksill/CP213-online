{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP213: Tutorial Notebook S2 Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Directional derivatives and gradients can be expanded to include more than 2 variables. If $f(x,y,z) = x\\sin{(yz)}$:\n",
    "\n",
    "1. Find $\\nabla{f}$.\n",
    "2. Find the directional derivative, $D_{\\hat{u}}f$ at the point $(1,3,0)$ in the direction of $\\vec{u} = \\hat{\\imath} + 2\\hat{\\jmath} - \\hat{k}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solutions for Q1\n",
    "\n",
    "### Part 1\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla f=\\dfrac{\\partial f}{\\partial x}\\hat{\\imath}+\\dfrac{\\partial f}{\\partial y}\\hat{\\jmath}+\\dfrac{\\partial f}{\\partial z}\\hat{k}\n",
    "\\end{align*}\n",
    "            \n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial f}{\\partial x} &=\\sin yz\\\\\n",
    "\\dfrac{\\partial f}{\\partial y} &=xz\\cos yz\\\\\n",
    "\\dfrac{\\partial f}{\\partial z} &=xy\\cos yz\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla f &= (\\sin yz)\\hat{\\imath} + (xz\\cos yz)\\hat{\\jmath} + (xy\\cos yz)\\hat{k}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "### Part 2\n",
    "\n",
    "At the point $P=(1,3,0)$\n",
    "\\begin{align*}\n",
    "    \\nabla f &= 0\\hat{\\imath}+ 0\\hat{\\jmath} + 3\\hat{k} \\\\\n",
    "    \\hat{u} = \\frac{\\vec{u}}{\\left| \\vec{u} \\right|} &= \\frac{\\hat{\\imath} + 2\\hat{\\jmath} - \\hat{k}}{\\sqrt{1^2 + 2^2 +(-1)^2}}\\\\\n",
    "    &= \\frac{1}{\\sqrt{6}}\\hat{\\imath} + \\frac{2}{\\sqrt{6}}\\hat{\\jmath} - \\frac{1}{\\sqrt{6}}\\hat{k}\\\\\n",
    "    D_{\\hat{u}}f &=\\nabla f \\cdot \\hat{u}\\\\\n",
    "    &= -\\frac{3}{\\sqrt{6}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import IPython.display as disp\n",
    "\n",
    "P = [1, 3, 0]\n",
    "u = sp.Matrix([1, 2, -1])\n",
    "\n",
    "x, y, z = sp.symbols('x y z')\n",
    "\n",
    "f = x*sp.sin(y*z)\n",
    "\n",
    "def gradient(f):\n",
    "    return(sp.diff(f, x), sp.diff(f, y), sp.diff(f,z))\n",
    "\n",
    "gradf = sp.Matrix(gradient(f)).subs([(x, P[0]), (y, P[1]), (z, P[2])])\n",
    "\n",
    "direction = u.normalized()\n",
    "\n",
    "Duf = gradf.dot(direction)\n",
    "\n",
    "disp.display(Duf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Consider a mathematically-gifted and lazy housefly which is manoeuvring through your kitchen. It begins its journey at the point $P=(1,1,1)$ and wishes to move in the direction which will result in the  greatest _decrease_ in temperature. If the temperature is given by:\n",
    "\\begin{align*}\n",
    "T(x,y,z) &= x^2 + 2y^2 +2z^2\n",
    "\\end{align*}\n",
    "\n",
    "1. In which direction should the housefly move in order that it achieves the greatest reduction in temperature with minimal change in position?\n",
    "2. What is the directional derivative at this point?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solutions for Q2\n",
    "\n",
    "### Part 1\n",
    "\n",
    "The greatest reduction in temperature will occur in the opposite direction (negative) of the gradient of the temperature, $\\nabla T$ at the point P:\n",
    "\\begin{align*}\n",
    "    -\\nabla T &= -\\left(2\\hat{\\imath} + 4\\hat{\\jmath} + 4 \\hat{k}\\right) \\\\\n",
    "    &= -2\\hat{\\imath} - 4\\hat{\\jmath} - 4 \\hat{k}\n",
    "\\end{align*}\n",
    "The associated unit vector in this direction is given by:\n",
    "\\begin{align*}\n",
    "    \\hat{u} = -\\frac{1}{3}\\hat{\\imath} - \\frac{2}{3}\\hat{\\jmath} - \\frac{2}{3}\\hat{k}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### Part 2\n",
    "\n",
    "The directional derivative can be determined by considering the dot product:\n",
    "\\begin{align*}\n",
    "    D_{\\hat{u}}T &= \\left|\\nabla T\\right|\\cos{\\theta}\\\\\n",
    "    &= 6(-1) = -6\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import IPython.display as disp\n",
    "\n",
    "P = [1, 1, 1]\n",
    "\n",
    "x, y, z = sp.symbols('x y z')\n",
    "\n",
    "T = x**2 + 2*y**2 + 2*z**2\n",
    "\n",
    "def gradient(f):\n",
    "    return(sp.diff(f, x), sp.diff(f, y), sp.diff(f,z))\n",
    "\n",
    "gradT = sp.Matrix(gradient(T)).subs([(x, P[0]), (y, P[1]), (z, P[2])])\n",
    "\n",
    "direction = -gradT.normalized()\n",
    "\n",
    "DuT = gradT.dot(direction)\n",
    "\n",
    "disp.display(DuT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Find and classify all the stationary points for:\n",
    "\\begin{align*}\n",
    "f(x,y) &= (x^2 + y^2)^2 - 2(x^2-y^2) + 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solutions for Q3\n",
    "\n",
    "Begin by determining all associated first-order and second-order partial derivatives\n",
    "\\begin{align*}\n",
    "    f_x     &= 4x(x^2 + y^2 - 1)    \\\\\n",
    "    f_{xx}  &= 4(3x^2 + y^2 - 1)    \\\\\n",
    "    f_y     &= 4y(x^2 + y^2 + 1)    \\\\\n",
    "    f_{yy}  &= 4(x^2 + 3y^2 + 1)    \\\\\n",
    "    f_{xy}  &= 8xy                  \n",
    "\\end{align*}\n",
    "Critical points when $f_x = f_y = 0$.\n",
    "\n",
    "From $f_x=0$: $x=0$ and $x=\\pm\\sqrt{1-y^2}$.\n",
    "\n",
    "Substituting $x=0$ into $f_y=0$ gives $y=0$.\n",
    "\n",
    "Substituting $x=\\pm\\sqrt{1-y^2}$ into $f_y=0$ gives: $y=0$ (and $x=\\pm 1$)\n",
    "\n",
    "Therefore, stationary points at $(0,0)$, $(1,0)$ and $(-1,0)$.\n",
    "\n",
    "Now to classify using $\\det{H}=f_{xx}f_{yy}-f_{xy}^2$\n",
    "\n",
    "At $(0,0)$: $f_{xx}=-4$, $f_{yy}=4$, $f_{xy}=0$ and $\\det{H}=-16$. Therefore saddle point at $(0,0)$\n",
    "\n",
    "At $(1,0)$: $f_{xx}=8$, $f_{yy}=8$, $f_{xy}=0$ and $\\det{H}=64$. Therefore local minimum at $(1,0)$\n",
    "\n",
    "At $(-1,0)$: $f_{xx}=8$, $f_{yy}=8$, $f_{xy}=0$ and $\\det{H}=64$. Therefore local minimum at $(-1,0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import IPython.display as disp\n",
    "\n",
    "\n",
    "x, y = sp.symbols('x y', real = True)        # Add real = True to omit imaginary solutions\n",
    "\n",
    "f = (x**2 + y**2)**2 - 2*(x**2 - y**2) + 1\n",
    "\n",
    "f_x = sp.simplify(sp.diff(f, x))\n",
    "f_y = sp.simplify(sp.diff(f, y))\n",
    "\n",
    "f_xx = sp.simplify(sp.diff(f_x, x))\n",
    "f_xy = sp.simplify(sp.diff(f_x, y))\n",
    "\n",
    "#f_yx = sp.simplify(sp.diff(f_y, x))\n",
    "f_yy = sp.simplify(sp.diff(f_y, y))\n",
    "\n",
    "StatPoints = []\n",
    "StatPoints = sp.solve((f_x, f_y), [x,y])\n",
    "\n",
    "def Hess(x,y):\n",
    "    detH = f_xx*f_yy - f_xy**2\n",
    "    return detH\n",
    "\n",
    "ClassSP = []\n",
    "\n",
    "for (X,Y) in StatPoints:\n",
    "    Hessian = (Hess(x,y))\n",
    "    ClassSP.append(Hessian.subs([(x, X), (y, Y)]))\n",
    "\n",
    "disp.display(f_x, f_xx, f_xy, f_y, f_yy,)\n",
    "print('Stationary Points = ', StatPoints)\n",
    "print('detH = ', ClassSP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "From linear regression, the function to be minimised, is the sum square of residuals, given by:\n",
    "\\begin{align*}\n",
    "S(\\beta_0,\\beta_1) \n",
    "&= \\sum_{i=1}^n \\left( y_i - \\beta_0 - \\beta_1 x_i \\right)^2\n",
    "\\end{align*}\n",
    "Derive expressions for the least-square regression coefficients $\\beta_0$ and $\\beta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solutions for Q4\n",
    "\n",
    "We must begin by considering that the sum of square of residuals will be at a critical point (minima) when:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial{S}}{\\partial{\\beta_0}}=\\frac{\\partial{S}}{\\partial{\\beta_1}}=0\n",
    "\\end{align*}\n",
    "Begin with consideration of $\\beta_0$. Please note that throughout this derivation, we will assume $\\sum$ is representative of $\\sum_{i=1}^n$.\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial{S}}{\\partial{\\beta_0}} = \\sum -2\\left(y_i - \\beta_0 - \\beta_1x_i\\right) = 0\\\\\n",
    "    \\sum \\left(y_i - \\beta_0 - \\beta_1x_i\\right) = 0\\\\\n",
    "    \\sum \\beta_0 = \\sum y_i -\\beta_1 \\sum x_i\\\\\n",
    "    n \\beta_0 = \\sum y_i -\\beta_1 \\sum x_i\\\\\n",
    "    \\beta_0 = \\frac{1}{n}\\sum y_i -\\beta_1 \\frac{1}{n}\\sum x_i\\\\\n",
    "\\end{align*}\n",
    "We know that $\\frac{1}{n}\\sum y_i=\\bar{y}$ and $\\frac{1}{n}\\sum x_i=\\bar{x}$, hence:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\beta_0 &= \\bar{y} -\\beta_1 \\bar{x}\n",
    "\\end{align*}\n",
    "\n",
    "Next, consider $\\beta_1$:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial{S}}{\\partial{\\beta_1}} = \\sum -2x_i\\left(y_i - \\beta_0 - \\beta_1x_i\\right) = 0                 \n",
    "    \\sum x_i\\left(y_i - \\beta_0 - \\beta_1x_i\\right) \n",
    "    \\sum x_iy_i - \\sum x_i\\beta_0 - \\sum\\beta_1x_i^2 = 0 \n",
    "\\end{align*}\n",
    "Substituting the expression for $\\beta_0$ in terms of $\\beta_1$, we find\n",
    "\\begin{align*}\n",
    "    \\sum x_iy_i - \\sum x_i\\left(\\bar{y} -\\beta_1 \\bar{x}\\right) - \\sum\\beta_1x_i^2 = 0\\\\ \n",
    "    \\sum x_iy_i - \\bar{y}\\sum x_i + \\beta_1\\bar{x}\\sum  x_i - \\beta_1\\sum x_i^2 = 0 \\\\\n",
    "    \\beta_1 \\bar{x}\\sum x_i - \\beta_1\\sum x_i^2 =  \\bar{y}\\sum x_i - \\sum x_iy_i\\\\\n",
    "    \\beta_1\\left(\\bar{x}\\sum x_i - \\sum x_i^2\\right) =  \\bar{y}\\sum x_i - \\sum x_iy_i\\\\ \n",
    "    \\beta_1 =  \\frac{\\bar{y}\\sum x_i - \\sum x_iy_i}{\\bar{x}\\sum x_i - \\sum x_i^2}\\\\\n",
    "\\end{align*}\n",
    "This satisfies the question, but we can make use of some identities and algebra to obtain a more familiar form that we made use of in statistics.\n",
    "\n",
    "These two identities will help:\n",
    "\\begin{align*}\n",
    "    \\sum\\left(x_i - \\bar{x}\\right)^2 &= \\sum x_i^2 - n\\bar{x}^2\\\\\n",
    "    \\sum\\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right) &= \\sum x_iy_i - n\\bar{x}\\bar{y}\n",
    "\\end{align*}\n",
    "Now considering equation for $\\beta_1$, we can multiply both the numerator and denominator by $-\\left(\\frac{n}{n}\\right)$. This gives us:\n",
    "\\begin{align*}\n",
    "    \\beta_1 =  \\frac{\\sum x_iy_i - \\frac{n}{n}\\bar{y}\\sum x_i}{\\sum x_i^2 - \\frac{n}{n}\\bar{x}\\sum x_i}\n",
    "\\end{align*}\n",
    "This cleverly gives us:\n",
    "\\begin{align*}\n",
    "    \\beta_1 =  \\frac{\\sum x_iy_i - n\\bar{y}\\bar{x}}{\\sum x_i^2 - \\bar{x}^2}\n",
    "\\end{align*}\n",
    "Using the two identities, we find:\n",
    "\\begin{align*}\n",
    "    \\beta_1 =\\frac{\\sum\\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)}{\\sum\\left(x_i - \\bar{x}\\right)^2}\n",
    "\\end{align*}\n",
    "With an explicit expression for $\\beta_1$, we can determine $\\beta_0$ from\n",
    "\\begin{align*}\n",
    "\\beta_0 \n",
    "&= \\bar{y} -\\beta_1 \\bar{x} .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
